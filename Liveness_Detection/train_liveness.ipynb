{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend, so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Agg is used for writing files\n",
    "\n",
    "from livenessnet import LivenessNet # our model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-d', '--dataset', required=True,\n",
    "                    help='Path to input Dataset')\n",
    "parser.add_argument('-m', '--model', type=str, required=True,\n",
    "                    help='Path to output trained model')\n",
    "parser.add_argument('-l', '--le', type=str, required=True,\n",
    "                    help='Path to output Label Encoder')\n",
    "parser.add_argument('-p', '--plot', type=str, default='plot.png',\n",
    "                    help='Path to output loss/accuracy plot')\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of images in out dataset directory\n",
    "# then, initialize the list of data and class of images\n",
    "print('[INFO] loading images...')\n",
    "imagePaths = list(paths.list_images(args['dataset'])) # get all image path from dataset path\n",
    "data = list()\n",
    "labels = list()\n",
    "\n",
    "# iterate over all image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label from file name, load and resize to 32x32\n",
    "    # example for our path '/face liveness detection/dataset\\\\fake\\\\0.png'\n",
    "    # so, if we split with os.path.sep we will get\n",
    "    # ['/face liveness detection/dataset', 'fake', '0.png']\n",
    "    # that means if we get the second last element we will get each image's label!\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (32,32)) # we will use 32x32 input shape for our model (not too big)\n",
    "    \n",
    "    # append the image and label to the list\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    \n",
    "# convert to ndarray and do feature scaling\n",
    "# tf works best with ndarray and it's super fast and efficient!\n",
    "data = np.array(data, dtype='float') / 255.0\n",
    "\n",
    "# encode the labels from (fake, real) to  (0,1)\n",
    "# and do one-hot encoding\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "labels = tf.keras.utils.to_categorical(labels, 2) # one-hot encoding\n",
    "\n",
    "# train/test split\n",
    "# we go for traditional 80/20 split\n",
    "# Theoretically, we have small dataset we need test set to be a bit bigger\n",
    "# 75/25 or 70/30 split would be ideal, but from the trial and error\n",
    "# 80/20 gives a better result, so we go for it\n",
    "# Another thing to consider, since my dataset has only about 14 images of faces from card/solid image\n",
    "# so 80/20 has a higher chance that those images will be in training set rather than test set (and none on training set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "# this method from TF will do augmentation at runtime. So, it's quite handy\n",
    "aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, \n",
    "                         zoom_range=0.15,\n",
    "                         width_shift_range=0.2,\n",
    "                         height_shift_range=0.2,\n",
    "                         shear_range=0.15,\n",
    "                         horizontal_flip=True,\n",
    "                         fill_mode='nearest')\n",
    "\n",
    "# build a model\n",
    "# define hyperparameters\n",
    "INIT_LR = 1e-4 # initial learning rate\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "\n",
    "# we don't need early stopping here because we have a small dataset, there is no need to do so\n",
    "# initialize the optimizer and model\n",
    "print('[INFO] compiling model...')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = LivenessNet.build(width=32, height=32, depth=3, classes=len(le.classes_))\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "# train the model\n",
    "print(f'[INFO] training model for {EPOCHS} epochs...')\n",
    "history = model.fit(x=aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "# evaluate the model\n",
    "print('[INFO] evaluating network...')\n",
    "predictions = model.predict(x=X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(classification_report(y_test.argmax(axis=1), predictions.argmax(axis=1), target_names=le.classes_))\n",
    "\n",
    "# save model to disk\n",
    "print(f\"[INFO serializing network to '{args['model']}']\")\n",
    "model.save(args['model'], save_format='h5')\n",
    "\n",
    "# save the label encoder to disk\n",
    "with open(args['le'], 'wb') as file:\n",
    "    file.write(pickle.dumps(le))\n",
    "    \n",
    "# plot training loss and accuract and save\n",
    "plt.style.use('ggplot')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), history.history['loss'], label='train_loss')\n",
    "plt.plot(np.arange(0, EPOCHS), history.history['val_loss'], label='val_loss')\n",
    "plt.plot(np.arange(0, EPOCHS), history.history['accuracy'], label='train_acc')\n",
    "plt.plot(np.arange(0, EPOCHS), history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Training Loss and Accuracy on Dataset')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend(loc='lower left')\n",
    "plt.savefig(args['plot'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
