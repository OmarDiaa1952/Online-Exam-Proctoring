{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# uncomment this line if you want to run your tensorflow model on CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to run this file from the shell,\n",
    "# uncomment these lines below and delete the function header and return\n",
    "\n",
    "# # construct the argument parser and parse the arguments\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-m', '--model', type=str, required=True,\n",
    "#                     help='Path to trained model')\n",
    "# parser.add_argument('-l', '--le', type=str, required=True,\n",
    "#                     help='Path to Label Encoder')\n",
    "# parser.add_argument('-d', '--detector', type=str, required=True,\n",
    "#                     help='Path to OpenCV\\'s deep learning face detector')\n",
    "# parser.add_argument('-c', '--confidence', type=float, default=0.5,\n",
    "#                     help='minimum probability to filter out weak detections')\n",
    "# parser.add_argument('-e', '--encodings', required=True,\n",
    "#                     help='Path to saved face encodings')\n",
    "# args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognition_liveness(model_path, le_path, detector_folder, encodings, confidence=0.5):\n",
    "    args = {'model':model_path, 'le':le_path, 'detector':detector_folder, \n",
    "            'encodings':encodings, 'confidence':confidence}\n",
    "\n",
    "    # load the encoded faces and names\n",
    "    print('[INFO] loading encodings...')\n",
    "    with open(args['encodings'], 'rb') as file:\n",
    "        encoded_data = pickle.loads(file.read())\n",
    "    # load our serialized face detector from disk\n",
    "    print('[INFO] loading face detector...')\n",
    "    proto_path = os.path.sep.join([args['detector'], 'deploy.prototxt'])\n",
    "    model_path = os.path.sep.join([args['detector'], 'res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "    detector_net = cv2.dnn.readNetFromCaffe(proto_path, model_path)\n",
    "    \n",
    "    # load the liveness detector model and label encoder from disk\n",
    "    liveness_model = tf.keras.models.load_model(args['model'])\n",
    "    le = pickle.loads(open(args['le'], 'rb').read())\n",
    "    \n",
    "    # initialize the video stream and allow camera to warmup\n",
    "    print('[INFO] starting video stream...')\n",
    "    vs = VideoStream(src=0).start()\n",
    "    time.sleep(2) # wait camera to warmup\n",
    "    # count the sequence that person appears\n",
    "    # this is just to make sure of that person and to show how model works\n",
    "    # you can delete this if you want\n",
    "    sequence_count = 0 \n",
    "    \n",
    "    # initialize variables needed to return\n",
    "    # in case, users press 'q' before the program process the frame\n",
    "    name = 'Unknown'\n",
    "    label_name = 'fake'\n",
    "    \n",
    "    # iterate over the frames from the video stream\n",
    "    while True:\n",
    "        # grab the frame from the threaded video stream\n",
    "        # and resize it to have a maximum width of 600 pixels\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "        cv2.putText(frame, \"Press 'q' to quit\", (20,35), cv2.FONT_HERSHEY_COMPLEX, 0.75, (0,255,0), 2)\n",
    "        # grab the frame dimensions and convert it to a blob\n",
    "        # blob is used to preprocess image to be easy to read for NN\n",
    "        # basically, it does mean subtraction and scaling\n",
    "        # (104.0, 177.0, 123.0) is the mean of image in FaceNet\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        \n",
    "        # pass the blob through the network \n",
    "        # and obtain the detections and predictions\n",
    "        detector_net.setInput(blob)\n",
    "        detections = detector_net.forward()\n",
    "        \n",
    "        # iterate over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e. probability) associated with the prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            # filter out weak detections\n",
    "            if confidence > args['confidence']:\n",
    "                # compute the (x,y) coordinates of the bounding box\n",
    "                # for the face and extract the face ROI\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype('int')\n",
    "                \n",
    "                # expand the bounding box a bit\n",
    "                # (from experiment, the model works better this way)\n",
    "                # and ensure that the bounding box does not fall outside of the frame\n",
    "                startX = max(0, startX-20)\n",
    "                startY = max(0, startY-20)\n",
    "                endX = min(w, endX+20)\n",
    "                endY = min(h, endY+20)\n",
    "                \n",
    "                # extract the face ROI and then preprocess it\n",
    "                # in the same manner as our training data\n",
    "                face = frame[startY:endY, startX:endX] # for liveness detection\n",
    "                # expand the bounding box so that the model can recog easier\n",
    "                face_to_recog = face # for recognition\n",
    "                # some error occur here if my face is out of frame and comeback in the frame\n",
    "                try:\n",
    "                    face = cv2.resize(face, (32,32)) # our liveness model expect 32x32 input\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "                # face recognition\n",
    "                rgb = cv2.cvtColor(face_to_recog, cv2.COLOR_BGR2RGB)\n",
    "                #rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "                encodings = face_recognition.face_encodings(rgb)\n",
    "                # initialize the default name if it doesn't found a face for detected faces\n",
    "                name = 'Unknown'\n",
    "                # loop over the encoded faces (even it's only 1 face in one bounding box)\n",
    "                # this is just a convention for other works with this kind of model\n",
    "                for encoding in encodings:\n",
    "                    matches = face_recognition.compare_faces(encoded_data['encodings'], encoding)\n",
    "                    \n",
    "                    # check whether we found a matched face\n",
    "                    if True in matches:\n",
    "                        # find the indexes of all matched faces then initialize a dict\n",
    "                        # to count the total number of times each face was matched\n",
    "                        matchedIdxs = [i for i, b in enumerate(matches) if b]\n",
    "                        counts = {}\n",
    "                        \n",
    "                        # loop over matched indexes and count\n",
    "                        for i in matchedIdxs:\n",
    "                            name = encoded_data['names'][i]\n",
    "                            counts[name] = counts.get(name, 0) + 1\n",
    "                            \n",
    "                        # get the name with the most count\n",
    "                        name = max(counts, key=counts.get)\n",
    "                            \n",
    "                face = face.astype('float') / 255.0 \n",
    "                face = tf.keras.preprocessing.image.img_to_array(face)\n",
    "                # tf model require batch of data to feed in\n",
    "                # so if we need only one image at a time, we have to add one more dimension\n",
    "                # in this case it's the same with [face]\n",
    "                face = np.expand_dims(face, axis=0)\n",
    "            \n",
    "                # pass the face ROI through the trained liveness detection model\n",
    "                # to determine if the face is 'real' or 'fake'\n",
    "                # predict return 2 value for each example (because in the model we have 2 output classes)\n",
    "                # the first value stores the prob of being real, the second value stores the prob of being fake\n",
    "                # so argmax will pick the one with highest prob\n",
    "                # we care only first output (since we have only 1 input)\n",
    "                preds = liveness_model.predict(face)[0]\n",
    "                j = np.argmax(preds)\n",
    "                label_name = le.classes_[j] # get label of predicted class\n",
    "                \n",
    "                # draw the label and bounding box on the frame\n",
    "                label = f'{label_name}: {preds[j]:.4f}'\n",
    "                if name == 'Unknown' or label_name == 'fake':\n",
    "                    sequence_count = 0\n",
    "                else:\n",
    "                    sequence_count += 1\n",
    "                print(f'[INFO] {name}, {label_name}, seq: {sequence_count}')\n",
    "                \n",
    "                if label_name == 'fake':\n",
    "                    cv2.putText(frame, \"Don't try to Spoof !\", (startX, endY + 25), \n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 0.7, (0,0,255), 2)\n",
    "                \n",
    "                cv2.putText(frame, name, (startX, startY - 35), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0,130,255),2 )\n",
    "                cv2.putText(frame, label, (startX, startY - 10),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 4)\n",
    "            \n",
    "        # show the output fame and wait for a key press\n",
    "        cv2.imshow('Frame', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # if 'q' is pressed, stop the loop\n",
    "        # if that person appears 10 frames in a row, stop the loop\n",
    "        # you can change this if your GPU run faster\n",
    "        if key == ord('q') or sequence_count==10:\n",
    "            break\n",
    "        \n",
    "    # cleanup\n",
    "    vs.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    # have some times for camera and CUDA to close normally\n",
    "    # (it can f*ck up GPU sometimes if you don't have high performance GPU like me LOL)\n",
    "    time.sleep(2)\n",
    "    return name, label_name\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    name, label_name = recognition_liveness('liveness.model', 'label_encoder.pickle', \n",
    "                                            'face_detector', '../face_recognition/encoded_faces.pickle', confidence=0.5)\n",
    "    print(name, label_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
